{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yelpapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/8g5lhnqx70g_0z7_k3mg4c140000gn/T/ipykernel_49924/509860617.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myelpapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYelpAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yelpapi'"
     ]
    }
   ],
   "source": [
    "from yelpapi import YelpAPI\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "\n",
    "API_KEY = '3HutAvbl1TRuIXG0mjgNjTGPuLy_ZhoON7UEYzJqG6o2D6gPkIVZr1RT4x3TBpR3iXtJ9lBnis50H7MXRp0uSisapP-8Tup98qEipjCL6AjzLv2jHM6UhMXZN8IBYnYx'\n",
    "yelp_api = YelpAPI(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: create list of neighborhoods for each midwest city\n",
    "cities = ['Chicago, Illinois',\n",
    " 'Columbus, Ohio', \n",
    " 'Indianapolis, Indiana',\n",
    " 'Detroit, Michigan',\n",
    " 'Milwaukee, Wisconsin',\n",
    " 'Kansas City, Missouri',\n",
    " 'St. Louis, Missouri',\n",
    " 'Minneapolis, Minnesota',\n",
    " 'Saint Paul, Minnesota']    # changed St. Paul to Saint Paul\n",
    "\n",
    "# scraping wikipedia page\n",
    "# https://stackoverflow.com/questions/23013220/max-retries-exceeded-with-url-in-requests\n",
    "page = ''\n",
    "while page == '':\n",
    "    try:\n",
    "        page = requests.get('https://en.wikipedia.org/wiki/Lists_of_neighborhoods_by_city#United_States')\n",
    "        break\n",
    "    except:\n",
    "        print(\"Connection refused by the server..\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "        \n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "content = soup.find('div', class_='mw-parser-output')\n",
    "items = content.find_all('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Chicago, Illinois': '/wiki/Neighborhoods_of_Chicago',\n",
       " 'Columbus, Ohio': '/wiki/Neighborhoods_in_Columbus,_Ohio',\n",
       " 'Detroit, Michigan': '/wiki/Neighborhoods_in_Detroit',\n",
       " 'Indianapolis, Indiana': '/wiki/Neighborhoods_of_Indianapolis',\n",
       " 'Milwaukee, Wisconsin': '/wiki/Neighborhoods_of_Milwaukee',\n",
       " 'Minneapolis, Minnesota': '/wiki/Neighborhoods_of_Minneapolis',\n",
       " 'Saint Paul, Minnesota': '/wiki/Neighborhoods_of_Saint_Paul',\n",
       " 'St. Louis, Missouri': '/wiki/Neighborhoods_of_St._Louis',\n",
       " 'Kansas City, Missouri': '/wiki/Neighborhoods_of_Kansas_City,_Missouri'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1.1: get directories (what comes after wikipedia base url) for all cities\n",
    "# creating dict where keys are cities and values are directories\n",
    "city_directories = {}\n",
    "\n",
    "# iterate through list of cities on wikipedia page - if they match our cities, get its directory\n",
    "for item in items:\n",
    "    wiki_city_cleaned = item.text.strip('*').strip()\n",
    "    if wiki_city_cleaned == 'Neighborhoods of Chicago':   # hard-coded\n",
    "        city_directories[cities[0]] = item.a['href']\n",
    "    for city in cities:\n",
    "        if wiki_city_cleaned == city.split(',')[0] or wiki_city_cleaned == city: \n",
    "            city_directories[city] = item.a['href']\n",
    "\n",
    "city_directories['Kansas City, Missouri'] = '/wiki/Neighborhoods_of_Kansas_City,_Missouri'   # hard-coded\n",
    "city_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1.2: creating functions for scraping wikipedia page & cleaning\n",
    "\n",
    "base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "# uses pandas read_html to scrape neighborhoods from table on each wiki page\n",
    "def read_wiki(city):\n",
    "    directory = city_directories.get(city)\n",
    "    wiki = pd.read_html(base_url + directory)\n",
    "    return wiki\n",
    "\n",
    "# uses bs4 to scrape neighborhoods based on html tags\n",
    "def soup(city):\n",
    "    directory = city_directories.get(city)\n",
    "    page = requests.get(base_url + directory)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    content = soup.find('div', class_='mw-parser-output')\n",
    "    return content\n",
    "\n",
    "# cleans where neighborhoods have multiple names with '/'\n",
    "# NOTE: still in progress\n",
    "def cleaned(lst):\n",
    "    new_lst = []\n",
    "    for neighborhood in lst:\n",
    "        if re.search(r\"^[\\w\\s]+\\s?[(\\/\\[]\", neighborhood):\n",
    "            if \"/\" in neighborhood:\n",
    "                towns = neighborhood.split(\"/\")\n",
    "                stripped_towns = [i.strip() for i in towns]\n",
    "                new_lst.extend(stripped_towns)\n",
    "            elif \"[\" in neighborhood:\n",
    "                matches = re.findall(r\"^([\\w\\s]+)\\s?[(\\/\\[]\", neighborhood)\n",
    "                stripped_matches = [i.strip() for i in matches]\n",
    "                if matches:\n",
    "                    new_lst.extend(stripped_matches)\n",
    "            else:\n",
    "                new_neighborhood = neighborhood.replace(\"(\", \"\").replace(\")\", \"\").strip()\n",
    "                new_lst.append(new_neighborhood)\n",
    "        else:\n",
    "            new_lst.append(neighborhood.strip())\n",
    "    return new_lst\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for neighborhood in lst:\n",
    "#         if '/' in neighborhood or ' / ' in neighborhood:\n",
    "#             lst.remove(neighborhood)\n",
    "#             aliases = neighborhood.split('/')\n",
    "#             for item in aliases:\n",
    "#                 lst.append(item.strip())\n",
    "#         if '[' in neighborhood:\n",
    "#             lst.remove(neighborhood)\n",
    "#             split = neighborhood.split('[')\n",
    "#             lst.append(split[0].strip())\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bricktown Historic District', 'Broadway Avenue Historic District', 'Campus Martius Park', 'Capitol Park Historic District', 'Detroit Financial District', 'Grand Circus Park Historic District', 'Greektown', 'Jefferson Avenue', 'Lower Woodward Avenue Historic District', 'Monroe Avenue Commercial Buildings', 'Park Avenue Historic District', 'Randolph Street Commercial Buildings Historic District', 'Washington Boulevard Historic District', 'Brush Park', 'Woodward East', 'Cass Corridor', 'Cass Park Historic District', 'Cass-Davenport Historic District', 'Cultural Center Historic District', 'East Ferry Avenue Historic District', 'Midtown Woodward Historic District', 'Sugar Hill Historic District', 'University-Cultural Center', 'Warren-Prentis Historic District', 'Wayne State University', 'West Canfield Historic District', 'Willis-Selden Historic District', 'Woodbridge', 'Arden Park-East Boston Historic District', 'Atkinson Avenue Historic District', 'Boston-Edison Historic District', 'New Amsterdam Historic District', 'New Center', 'Piquette Avenue Industrial Historic District', 'Virginia Park Historic District', 'Chaldean Town', 'Green Acres', 'Grixdale Farms', 'Highland Park', 'Palmer Park Apartment Building Historic District', 'Palmer Woods', 'Sherwood Forest', 'University District', 'University of Detroit Mercy', 'Chandler Park', 'Cornerstone Village', 'East English Village', 'Hamtramck', 'Krainz Woods', 'Milwaukee Junction', 'MorningSide', 'Van Steuban', 'Osborn', 'NoHam', 'Banglatown', 'Eastern Market', 'Forest Park', 'Poletown East', 'Belle Isle State Park', 'St. Charles Borromeo Roman Catholic Parish Complex', 'Eastside Historic Cemetery District', 'East Grand Boulevard Historic District', 'East Jefferson Avenue Residential District', 'Indian Village', 'Island View', 'Jefferson-Chalmers Historic Business District', 'Lafayette Park', 'Mies van der Rohe Residential District', 'Rivertown', 'West Village', 'Bagley', 'Grandmont', 'Martin Park', 'Old Redford', 'Rosedale Park', 'Greenwich Park', 'Parkland', 'Warrendale', 'Corktown', 'Delray', 'Hubbard Farms', 'North Corktown', 'Mexicantown', 'Springwells', 'Westside Industrial', 'West Vernor–Junction Historic District', 'West Vernor–Lawndale Historic District', 'West Vernor–Springwells Historic District', \"Highland Heights-Stevens' Subdivision Historic District \", ' North End']\n"
     ]
    }
   ],
   "source": [
    "# STEP 1.3: get neighborhoods for chicago, indianapolis, detroit, st. louis\n",
    "# creating dict where key is city and value is list of neighborhoods\n",
    "neighborhoods = {}\n",
    "\n",
    "# chicago neighborhoods\n",
    "wiki_chicago = read_wiki('Chicago, Illinois')\n",
    "chicago_neighborhoods = wiki_chicago[0]\n",
    "chicago_neighborhoods = list(chicago_neighborhoods['Neighborhood'])\n",
    "neighborhoods['Chicago, Illinois'] = chicago_neighborhoods\n",
    "\n",
    "# indianapolis neighborhoods\n",
    "wiki_ind = read_wiki('Indianapolis, Indiana')\n",
    "ind_neighborhoods = wiki_ind[2]\n",
    "indianapolis_neighborhoods = list(ind_neighborhoods[ind_neighborhoods.columns[0]])\n",
    "neighborhoods['Indianapolis, Indiana'] = indianapolis_neighborhoods\n",
    "\n",
    "# st. louis neighborhoods\n",
    "wiki_stl = read_wiki('St. Louis, Missouri')\n",
    "stl_neighborhoods = wiki_stl[1]\n",
    "stl_neighborhoods = list(stl_neighborhoods['Neighborhood'])\n",
    "neighborhoods['St. Louis, Missouri'] = stl_neighborhoods\n",
    "\n",
    "# detroit neighborhoods\n",
    "wiki_detroit =  read_wiki('Detroit, Michigan')\n",
    "detroit_neighborhoods = []\n",
    "for page in wiki_detroit:\n",
    "    if 'Name' in page.columns:\n",
    "        detroit_neighborhoods.extend(list(page['Name']))\n",
    "        \n",
    "# cleaning detroit neighborhoods\n",
    "detroit_neighborhoods = cleaned(detroit_neighborhoods)\n",
    "for n in detroit_neighborhoods:\n",
    "    if '/' in n:\n",
    "        detroit_neighborhoods.remove(n)\n",
    "        names = n.split('/')\n",
    "        for name in names:\n",
    "            detroit_neighborhoods.append(name)\n",
    "neighborhoods['Detroit, Michigan'] = detroit_neighborhoods\n",
    "\n",
    "print(detroit_neighborhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1.4: get neighborhoods for columbus, milwaukee, kansas city, minneapolis, saint paul \n",
    "# NOTE: need fix repetition of code later for efficiency\n",
    "\n",
    "# columbus neighborhoods\n",
    "columbus_directory = city_directories.get('Columbus, Ohio')\n",
    "page = requests.get(base_url + columbus_directory)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "content = soup.find('div', class_='mw-parser-output')\n",
    "columbus_neighborhoods = [item.text.replace('[edit]', '') for item in content.find_all(['h2', 'h3', 'h4'])][2:-3]\n",
    "neighborhoods['Columbus, Ohio'] = columbus_neighborhoods\n",
    "\n",
    "# milwaukee neighborhoods\n",
    "mlwk_directory = city_directories.get('Milwaukee, Wisconsin')\n",
    "page = requests.get(base_url + mlwk_directory)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "content = soup.find('div', class_='mw-parser-output')\n",
    "mlwk_neighborhoods = [item.text.replace('[edit]', '').strip('\"') for item in content.find_all('h3')]\n",
    "neighborhoods['Milwaukee, Wisconsin'] = mlwk_neighborhoods\n",
    "\n",
    "# kansas neighborhoods\n",
    "kansas_directory = city_directories.get('Kansas City, Missouri')\n",
    "page = requests.get(base_url + kansas_directory)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "content = soup.find('div', class_='mw-parser-output')\n",
    "li_list = [item.text for item in content.find_all('li')]\n",
    "kansas_neighborhoods = li_list[li_list.index('CBD-Downtown'):li_list.index('Wornall Homestead') + 1]\n",
    "neighborhoods['Kansas City, Missouri'] = kansas_neighborhoods\n",
    "\n",
    "# minneapolis neightborhoods\n",
    "minneapolis_directory = city_directories.get('Minneapolis, Minnesota')\n",
    "page = requests.get(base_url + minneapolis_directory)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "content = soup.find('div', class_='mw-parser-output')\n",
    "h3 = [item.text.replace('[edit]', '').strip('\"') for item in content.find_all(['h3'])]\n",
    "li = [item.text for item in content.find_all(['li']) if item.text[0].isalpha()]\n",
    "li = li[0:li.index('University') + 1]\n",
    "b = [item.text for item in content.find_all('b') if item.text[0].isalpha() and item.text not in li]\n",
    "minneapolis_neighborhoods = h3 + li + b\n",
    "neighborhoods['Minneapolis, Minnesota'] = minneapolis_neighborhoods\n",
    "\n",
    "# saint paul neighborhoods\n",
    "stpaul_directory = city_directories.get('Saint Paul, Minnesota')\n",
    "page = requests.get(base_url + stpaul_directory)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "content = soup.find('div', class_='mw-parser-output')\n",
    "h3_list = [item.text.strip().split('- ')[1].replace('[edit]', '') for item in content.find_all('h3')]\n",
    "stpaul_neighborhoods = []\n",
    "for h3 in h3_list:\n",
    "    names = h3.split(',')\n",
    "    for name in names:\n",
    "        stpaul_neighborhoods.append(name.strip())\n",
    "neighborhoods['Saint Paul, Minnesota'] = stpaul_neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Detroit, Michigan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/8g5lhnqx70g_0z7_k3mg4c140000gn/T/ipykernel_49924/2593344509.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# how many restaurants do we want for each neighborhood? aka what limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneighborhoods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Detroit, Michigan'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myelp_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'restaurants'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', Detroit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Detroit, Michigan'"
     ]
    }
   ],
   "source": [
    "# step 2: use yelp API to get restaurant data for each neighborhood\n",
    "# starting with detroit\n",
    "# how many restaurants do we want for each neighborhood? aka what limit\n",
    "\n",
    "for n in neighborhoods['Detroit, Michigan']:\n",
    "    try:\n",
    "        print(yelp_api.search_query(categories='restaurants', location=n + ', Detroit', limit=1))\n",
    "    except YelpAPI.YelpAPIError as e:\n",
    "        print('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: write results into csv file (or create pandas dataframe?) for each neighborhood"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
